---
# This script provisions machines and deploys JupyterHub on them, as well as
# setting up a network file system mirrored across all instances.
#
# Note that this script does a few things which are simple rather than using
#   statuses to get definitive results. This was done in the interest of making
#   better use of time allotted, mainly creating and checking files instead of
#   running commands to check statuses. Most of these can probably be simply
#   replaced without an immense amount of hassle, but the script should set up
#   the system anyway, so there shouldn't be any harm done by leaving it as it
#   is.
- name: Check OpenStack image facts for personal project.
  hosts: localhost
  #vars:

  vars_files:
    - variables/vars.yml
  # - host_vars/nectar.yaml # Use the variables retrieved from open stack
                           # information.
  tasks:
    - name: Create volumes
      os_volume:
        api_timeout: 50 # Seconds(?)
        auth:
          auth_url: "{{ cloud_auth_url }}"
          username: "{{ cloud_username }}"
          password: "{{ cloud_password }}"
          project_name: "{{ cloud_project_name }}"
        display_name: "{{ volume_name_prefix }}{{ item }}{{ volume_name_suffix }}"
        display_description: "Disk #{{ item }}, created through ansible"
        availability_zone: "{{ volume_availability_zone }}" # Marked in docs as
                                                            # for backwards
                                                            # compatibility, but
                                                            # required by NeCTAR
        region_name: "{{ volume_region }}"
        size: "{{ volume_size_gb }}" # in GB
        state: present
        wait: yes # Wait until volume created
      with_sequence: start=1 end={{ volume_instances }}
      when: attach_volumes

    - name: Create security group
      os_security_group:
        api_timeout: 50 # Seconds(?)
        auth:
          auth_url: "{{ cloud_auth_url }}"
          username: "{{ cloud_username }}"
          password: "{{ cloud_password }}"
          project_name: "{{ cloud_project_name }}"
        description: "Handles JupyterHub permissions"
        name: "{{ security_group_name }}"
        state: present
        wait: yes

    # If this is not supported by a different cloud provider, you can simply
    # open required ports.
    - name: Open all ports between members of JupyterHub security group
      os_security_group_rule:
        api_timeout: 50 # Seconds(?)
        auth:
          auth_url: "{{ cloud_auth_url }}"
          username: "{{ cloud_username }}"
          password: "{{ cloud_password }}"
          project_name: "{{ cloud_project_name }}"
        direction: ingress
        ethertype: IPv4
        remote_group: "{{ security_group_name }}"
        security_group: "{{ security_group_name }}"
        wait: yes
        state: present

    - name: Open http port
      os_security_group_rule:
        api_timeout: 50
        auth:
          auth_url: "{{ cloud_auth_url }}"
          username: "{{ cloud_username }}"
          password: "{{ cloud_password }}"
          project_name: "{{ cloud_project_name }}"
        direction: ingress
        ethertype: IPv4
        protocol: tcp
        port_range_min: "{{ security_rule_interface_port }}"
        port_range_max: "{{ security_rule_interface_port }}"
        remote_ip_prefix: "0.0.0.0/0"
        security_group: "{{ security_group_name }}"
        wait: yes
        state: present

    - name: Open https port
      os_security_group_rule:
        api_timeout: 50
        auth:
          auth_url: "{{ cloud_auth_url }}"
          username: "{{ cloud_username }}"
          password: "{{ cloud_password }}"
          project_name: "{{ cloud_project_name }}"
        direction: ingress
        ethertype: IPv4
        protocol: tcp
        port_range_min: "{{ security_rule_interface_port_https }}"
        port_range_max: "{{ security_rule_interface_port_https }}"
        remote_ip_prefix: "0.0.0.0/0"
        security_group: "{{ security_group_name }}"
        wait: yes
        state: present

    - name: Open https negotiation port
      os_security_group_rule:
        api_timeout: 50
        auth:
          auth_url: "{{ cloud_auth_url }}"
          username: "{{ cloud_username }}"
          password: "{{ cloud_password }}"
          project_name: "{{ cloud_project_name }}"
        direction: ingress
        ethertype: IPv4
        protocol: tcp
        port_range_min: "{{ security_rule_interface_port_https_negotiation }}"
        port_range_max: "{{ security_rule_interface_port_https_negotiation }}"
        remote_ip_prefix: "0.0.0.0/0"
        security_group: "{{ security_group_name }}"
        wait: yes
        state: present

    # Creates the instance with required properties.
    - name: Create instances with attached volume.
      os_server:
        api_timeout: 3600 # Seconds(?)
        timeout: 3600
        auth:
          auth_url: "{{ cloud_auth_url }}"
          username: "{{ cloud_username }}"
          password: "{{ cloud_password }}"
          project_name: "{{ cloud_project_name }}"
        availability_zone: "{{ instance_availability_zone }}"
        flavor: "{{ instance_flavour }}"
        image: "{{ instance_image }}"
        key_name: "{{ instance_key_pair_name }}"
        name: "{{ instance_name_prefix }}{{ item }}{{ instance_name_suffix }}"
        network: "{{ instance_network }}"
        region_name: "{{ instance_region }}"
        security_groups: "{{ instance_security_groups }}"
        volumes:
          - "{{ volume_name_prefix }}{{ item }}{{ volume_name_suffix }}"
        state: present
        wait: yes # Wait up to timeout seconds until instance created
      with_sequence: start=1 end={{ volume_instances }}
      when: attach_volumes

    - name: Create instances without attached volume.
      os_server:
        api_timeout: 3600 # Seconds(?)
        timeout: 3600
        auth:
          auth_url: "{{ cloud_auth_url }}"
          username: "{{ cloud_username }}"
          password: "{{ cloud_password }}"
          project_name: "{{ cloud_project_name }}"
        availability_zone: "{{ instance_availability_zone }}"
        flavor: "{{ instance_flavour }}"
        image: "{{ instance_image }}"
        key_name: "{{ instance_key_pair_name }}"
        name: "{{ instance_name_prefix }}{{ item }}{{ instance_name_suffix }}"
        network: "{{ instance_network }}"
        region_name: "{{ instance_region }}"
        security_groups: "{{ instance_security_groups }}"
        state: present
        wait: yes # Wait up to timeout seconds until instance created
      with_sequence: start={{ volume_instances + 1 }} end={{ instance_count }}
      when: attach_volumes

    # Creates the instance with required properties (without volumes).
    - name: Create all instances without attached volume.
      os_server:
        api_timeout: 3600 # Seconds(?)
        timeout: 3600
        auth:
          auth_url: "{{ cloud_auth_url }}"
          username: "{{ cloud_username }}"
          password: "{{ cloud_password }}"
          project_name: "{{ cloud_project_name }}"
        availability_zone: "{{ instance_availability_zone }}"
        flavor: "{{ instance_flavour }}"
        image: "{{ instance_image }}"
        key_name: "{{ instance_key_pair_name }}"
        name: "{{ instance_name_prefix }}{{ item }}{{ instance_name_suffix }}"
        network: "{{ instance_network }}"
        region_name: "{{ instance_region }}"
        security_groups: "{{ instance_security_groups }}"
        state: present
        wait: yes # Wait up to timeout seconds until instance created
      with_sequence: start=1 end={{ instance_count }}
      when: not attach_volumes

    # Need this to work out IP addresses of all nodes to run on, any other
    # method will suffice.
    - name: Get instance facts
      os_server_facts:
        api_timeout: 50 # Seconds(?)
        auth:
          auth_url: "{{ cloud_auth_url }}"
          username: "{{ cloud_username }}"
          password: "{{ cloud_password }}"
          project_name: "{{ cloud_project_name }}"
        server: "{{ instance_name_prefix }}*"
        wait: yes

    # Uses above facts to set up list of all nodes to run on.
    - name: Add compute hosts to in-memory inventory
      add_host:
        groups: jupyterhub_nodes
        name: "{{ item['accessIPv4'] }}"
      loop: "{{ openstack_servers }}"

    # This node is arbitrary, but can handle anything where a single origin node
    # is required to manage system setup correctly.
    # NOTE: This _is_ idempotent in the presence of scaling. Nodes are retrieved
    # in descending order of creation date, hence the first element will remain
    # the first element _unless_ the first element is removed. If these
    # guarantees are not provided in other cloud APIs, more sophisticated
    # election mechanisms may be required.
    - name: Elect cluster setup manager node
      add_host:
        groups: jupyterhub_head_node
        name: "{{ (openstack_servers | first)['accessIPv4'] }}"

    # Store all nodes with storage attached.
    - name: Add hosts with storage attached to in-memory inventory
      add_host:
        groups: jupyterhub_storage_nodes
        name: "{{ item['accessIPv4'] }}"
      loop: "{{ openstack_servers }}"
      when: (item['properties']['os-extended-volumes:volumes_attached'] | length) > 0

    # NOTE: known-hosts module is not idempotent either, so simple shell script
    #   appears to be just as effective and less convoluted. Could also do a
    #   lineinfile I believe, but seems to be acceptable for Ansible at present
    #   not to.
    # Trust all the newly created servers.
    - name: Add fingerprints to known hosts file
      shell: "ssh-keyscan -H {{ item['accessIPv4'] }} >> ~/.ssh/known_hosts"
      loop: "{{ openstack_servers }}"

- name: Wait for all machines to become ready
  hosts: all
  gather_facts: no
  tasks:
    - name: Wait for machines
      wait_for_connection:
        sleep: 30
      retries: 3

- name: Prepare nodes for cluster
  hosts: jupyterhub_nodes
  become_user: root
  become: yes
  vars_files: variables/vars.yml

  # no_proxy content should be updated.
  tasks:
    # Note, this task hasn't been tested, if it breaks the script follow the
    # steps in the readme to restart this service manually.
    - name: Restart time sync service on all hosts
      become: yes
      systemd:
        daemon_reload: yes
        name: systemd-timesyncd.service
        state: restarted

    - name: Set up environment on all hosts
      lineinfile:
        path: /etc/environment
        line: "{{ item }}"
        state: present
      loop:
        - "http_proxy=\"http://wwwproxy.unimelb.edu.au:8000\""
        - "https_proxy=\"http://wwwproxy.unimelb.edu.au:8000\""
        - "ftp_proxy=\"http://wwwproxy.unimelb.edu.au:8000\""
        - "no_proxy=\"localhost,127.0.0.1,{{ inventory_hostname }},192.168.0.0/16,10.96.0.0/12,{{ groups['jupyterhub_nodes'] | join(',') }}\""

    - name: Set up environment on all hosts (.bashrc)
      lineinfile:
        path: /home/ubuntu/.bashrc
        line: "{{ item }}"
        state: present
      loop:
        - "http_proxy=\"http://wwwproxy.unimelb.edu.au:8000\""
        - "https_proxy=\"http://wwwproxy.unimelb.edu.au:8000\""
        - "ftp_proxy=\"http://wwwproxy.unimelb.edu.au:8000\""
        - "no_proxy=\"localhost,127.0.0.1,{{ inventory_hostname }},192.168.0.0/16,10.96.0.0/12,{{ groups['jupyterhub_nodes'] | join(',') }}\""

    # Assume disk to mount is final disk in block devices.
    - name: Get disk to mount
      shell: lsblk -o NAME | tail -n 1
      register: pv_dev

    # Can't resize online volume if the source and destination size are the same
    # it seems, might be a way to get around this, but I don't need to do that
    # and it's probably easier to resize using an additional playbook later.
    - name: Create filesystem for all volumes
      filesystem:
        dev: "/dev/{{ pv_dev['stdout_lines'][0] }}"
        fstype: ext4
      loop: "{{ groups['jupyterhub_storage_nodes'] }}"
      when: attach_volumes and item == hostvars[inventory_hostname]['ansible_default_ipv4']['address']

    # Note: This is done whether we have volumes attached or not.
    - name: Create directory to mount
      file:
        path: "{{ volume_mount_directory }}"
        state: directory

    - name: Mount volume directory
      mount:
        path: "{{ volume_mount_directory }}"
        src: "/dev/{{ pv_dev['stdout_lines'][0] }}"
        state: mounted
        fstype: ext4
      loop: "{{ groups['jupyterhub_storage_nodes'] }}"
      when: attach_volumes and item == hostvars[inventory_hostname]['ansible_default_ipv4']['address']

    - name: Give all permissions to volumes
      become: yes
      file:
        path: "{{ volume_mount_directory }}"
        # Might want to restrict this more, but go wild for now.
        mode: 777
      loop: "{{ groups['jupyterhub_storage_nodes'] }}"
      when: attach_volumes and item == hostvars[inventory_hostname]['ansible_default_ipv4']['address']

- name: Install required software on all nodes
  hosts: jupyterhub_nodes
  become_user: root
  become: yes
  vars_files: variables/vars.yml
  tasks:
    - name: Install apt-transport-https
      apt:
        state: present
        update_cache: yes
        name: apt-transport-https

    - name: Install docker
      apt:
        state: present
        name: docker

    - name: Install pip
      apt:
        state: present
        name: python3-pip

    - name: Install NFS server
      apt:
        state: present
        name: nfs-kernel-server

    - name: Install NFS client
      apt:
        state: present
        name: nfs-common

    # This is required by Kubernetes at the time of install. In the future this
    # directive may not be necessary.
    - name: Turn off swap for all devices
      shell: swapoff -a

    # Key here is that we switch from cgroupsfs to systemd as warned by kubeadm.
    - name: Set up docker configuration
      copy:
        src: roles/files/daemon.json
        dest: /etc/docker/daemon.json

    - name: Create folder for docker systemd configuration
      file:
        path: /etc/systemd/system/docker.service.d
        state: directory

    # Note: I haven't tested this in the script.
    - name: Shift Docker to ephemeral disk
      lineinfile:
        path: /etc/systemd/system/docker.service.d/docker.root.conf
        line: "{{ item }}"
        state: directory
      loop:
        - [Service]
        - ExecStart=
        - ExecStart=/usr/bin/dockerd -g /mnt/dockerd -H fd://
      when: use_ephemeral_docker

    - name: Restart docker
      systemd:
        daemon_reload: yes
        name: docker
        state: restarted

    - name: Install gpg key for Google
      apt_key:
        state: present
        url: https://packages.cloud.google.com/apt/doc/apt-key.gpg

    - name: Add repository for kubernetes
      lineinfile:
        path: /etc/apt/sources.list.d/kubernetes.list
        line: "deb https://apt.kubernetes.io/ kubernetes-xenial main"
        state: present
        create: yes

    - name: Update apt cache and install kubelet, kubeadm and kubectl
      apt:
        state: present
        update_cache: yes
        name: "{{ item }}"
      loop:
        - kubelet
        - kubectl
        - kubeadm

    - name: Restart kubelet
      systemd:
        daemon_reload: yes
        name: kubelet
        state: restarted

- name: Set up NFS on all hosts
  hosts: jupyterhub_nodes
  become_user: root
  become: yes
  vars_files: variables/vars.yml
  tasks:
    - name: Create directory for NFS share
      file:
        path: "{{ volume_mount_directory }}/{{ local_pv_directory }}/"
        state: directory
        mode: "777"
        owner: "nobody"
        group: "nogroup"

    - name: Allow all jupyterhub nodes to access shared files.
      lineinfile:
        path: /etc/exports
        create: yes
        line: "{{ volume_mount_directory }}/{{ local_pv_directory }}/ {{ item }}(rw,sync,no_subtree_check)"
        mode: "777"
        state: present
      loop: "{{ groups['jupyterhub_nodes'] }}"

    - name: Create directory for ephemeral NFS share
      file:
        path: "{{ ephemeral_volume_share_directory }}"
        state: directory
        mode: "777"
        owner: "nobody"
        group: "nogroup"

    - name: Allow all jupyterhub nodes to access ephemeral shared files.
      lineinfile:
        path: /etc/exports
        create: yes
        line: "{{ ephemeral_volume_share_directory }} {{ item }}(rw,sync,no_subtree_check)"
        mode: "777"
        state: present
      loop: "{{ groups['jupyterhub_nodes'] }}"

    - name: Allow all jupyterhub nodes to access shared ephemeral folders.
      lineinfile:
        path: /etc/exports
        create: yes
        line: "{{ volume_mount_directory }}/{{ local_pv_directory }}/ {{ item }}(rw,sync,no_subtree_check)"
        mode: "777"
        state: present
      loop: "{{ groups['jupyterhub_nodes'] }}"

    - name: Export filesystem
      command: exportfs -a
      become: yes

    - name: Restart NFS server
      systemd:
        daemon_reload: yes
        name: nfs-kernel-server
        state: restarted

    - name: Create directories for all NFS shares
      file:
        path: "{{ nfs_share_directory_prefix }}{{ item }}{{ nfs_share_directory_suffix }}"
        state: directory
        mode: "777"
        owner: "nobody"
        group: "nogroup"
      loop: "{{ groups['jupyterhub_nodes'] }}"

    - name: Create directories for all ephemeral NFS shares
      file:
        path: "{{ nfs_ephemeral_share_directory_prefix }}{{ item }}{{ nfs_ephemeral_share_directory_suffix }}"
        state: directory
        mode: "777"
        owner: "nobody"
        group: "nogroup"
      loop: "{{ groups['jupyterhub_nodes'] }}"

    - name: Mount NFS shares
      mount:
        path: "{{ nfs_share_directory_prefix }}{{ item }}{{ nfs_share_directory_suffix }}"
        src: "{{ item }}:{{ volume_mount_directory }}/{{ local_pv_directory }}/"
        state: mounted
        fstype: nfs
      loop: "{{ groups['jupyterhub_nodes'] }}"

- name: Set up cluster control-plane
  hosts: jupyterhub_head_node
  become_user: root
  vars_files: variables/vars.yml
  tasks:
    # This just creates ./kubeadm_initialised which can be used to check if we
    # have a cluster already up.
    - name: Check whether kubeadm already initialised
      command: touch ./kubeadm_initialised
      args:
        removes: /etc/kubernetes/manifests/etcd.yaml

    - name: Initialise control-plane node with kubeadm
      # Calico requires that we pass --pod-network-cidr=192.168.0.0/16 during
      # init.
      command: kubeadm init --pod-network-cidr=192.168.0.0/16
      args:
        # We only run this command once, it creates a number of files, but
        # checking one should suffice.
        creates: /etc/kubernetes/manifests/etcd.yaml
      become: yes

    - name: Create directory for root copy of config
      file:
        path: "$HOME/.kube/"
        state: directory
      become: yes

    - name: Copy kube config into root local directory
      command: "cp /etc/kubernetes/admin.conf $HOME/.kube/config"
      args:
        creates: "$HOME/.kube/admin.conf"
      become: yes

    # There's probably an easier way to do this, but this should do the trick.
    - name: Set up kube config directory script
      lineinfile:
        path: ./chownkube.sh
        create: yes
        line: "sudo chown $(id -u):$(id -g) $HOME/.kube/config"
        mode: "777"
        state: present

    - name: Take ownership of kube config directory
      command: sh ./chownkube.sh
      become: yes

    - name: Remove chownkube.sh
      file:
        path: ./chownkube.sh
        state: absent

    # -------------Add copy of config for user----------------------------------
    - name: Create directory for user copy of config
      file:
        path: "$HOME/.kube/"
        state: directory

    - name: Copy kube config into user local directory
      command: "cp /etc/kubernetes/admin.conf /home/{{ image_user }}/.kube/config"
      args:
        creates: "/home/{{ image_user }}/.kube/admin.conf"
      become: yes

    # There's probably an easier way to do this, but this should do the trick.
    - name: Set up kube config directory script for user
      lineinfile:
        path: ./chownkube.sh
        create: yes
        line: "sudo chown $(id -u):$(id -g) /home/{{ image_user }}/.kube/config"
        mode: "777"
        state: present
      become_user: "{{ image_user }}"

    - name: Take ownership of kube config directory for user
      command: sh ./chownkube.sh
      become: yes
      become_user: "{{ image_user }}"

    - name: Remove chownkube.sh
      file:
        path: ./chownkube.sh
        state: absent
    # --------------------------------------------------------------------------

    - name: Allow nodes to be scheduled on the control-plane machine
      command: kubectl taint nodes --all node-role.kubernetes.io/master-
      args:
        creates: ./kubeadm_initialised
      become: yes
      when: not untainted_control_node

    - name: Allow nodes to be scheduled on the control-plane machine
      command: kubectl taint nodes --all node.kubernetes.io/not-ready-
      args:
        creates: ./kubeadm_initialised
      become: yes

    # Calico is commonly used, I don't really have a strong reason to select
    # anything else, but https://kubedex.com/kubernetes-network-plugins/ has a
    # good comparison of different network providers and Weave Net may be an
    # option if security is of large concern.
    - name: Get Calico pod configuration
      get_url:
        dest: ./calico.yaml
        mode: "777"
        url: https://docs.projectcalico.org/v3.10/manifests/calico.yaml

    - name: Apply Calico pod configuration
      command: kubectl apply -f calico.yaml
      args:
        creates: ./kubeadm_initialised
      become: yes

    # We can't taint control nodes _before_ the network has come up, or the
    # network will fail to come up.
    - name: Wait for pods to come up
      shell: kubectl get pods --all-namespaces -o json
      register: kubectl_get_pods
      until: kubectl_get_pods.stdout|from_json|json_query('items[*].status.phase')|unique == ["Running"]
      delay: 30
      retries: 10
      become: yes

    # Use this because we might be joining additional nodes to the cluster.
    - name: Generate new token
      shell: kubeadm token create --print-join-command > kube_join_command.token
      become: yes

    # Copy token to local machine.
    - name: Copy token to localhost
      fetch:
        src: kube_join_command.token
        dest: token

- name: Join all nodes to cluster control-plane
  hosts: jupyterhub_nodes:!jupyterhub_head_node
  become_user: root
  vars_files: variables/vars.yml
  tasks:
    - name: Create a file to mark if we've already joined
      command: touch ./joined_kube_already
      args:
        removes: ./kube_join_command.token

    - name: Fetch token for joining network
      copy:
        src: "token/{{ groups['jupyterhub_head_node'] | first }}/kube_join_command.token"
        dest: ./
        mode: "777"

    - name: Join to kubeadm network
      command: sh ./kube_join_command.token
      args:
        creates: ./joined_kube_already
      become: yes

- name: Install helm on all nodes
  hosts: jupyterhub_nodes
  become_user: root
  vars_files: variables/vars.yml
  tasks:
    - name: Get Helm
      get_url:
        dest: ./helm_install.sh
        mode: "777"
        url: https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get

    - name: Install Helm
      command: bash ./helm_install.sh
      args:
        creates: ./helm_installed
      become: yes

    - name: Mark Helm Installed
      file:
        path: ./helm_installed
        state: touch

- name: Set up helm on master node
  hosts: jupyterhub_head_node
  become_user: root
  vars_files: variables/vars.yml
  tasks:
    - name: Create service account for tiller
      command: kubectl --namespace kube-system create serviceaccount tiller
      args:
        creates: ./helm_initialised
      become: yes

    - name: Give service account for tiller permissions to manage cluster
      command: kubectl create clusterrolebinding tiller --clusterrole cluster-admin --serviceaccount=kube-system:tiller
      args:
        creates: ./helm_initialised
      become: yes

    - name: Initialise helm and tiller
      command: helm init --service-account tiller --wait
      args:
        creates: ./helm_initialised
      become: yes

    - name: Mark Helm initialised
      file:
        path: ./helm_initialised
        state: touch

- name: Set up helm clients on other nodes
  hosts: jupyterhub_nodes:!jupyterhub_head_node
  become_user: root
  vars_files: variables/vars.yml
  tasks:
    - name: Initialise Helm for clients
      command: helm init --client-only
      args:
        creates: ./helm_initialised
      become: yes

    - name: Mark Helm initialised
      file:
        path: ./helm_initialised
        state: touch

- name: Secure helm on the master node
  hosts: jupyterhub_head_node
  become_user: root
  vars_files: variables/vars.yml
  tasks:
    - name: Apply patch to limit where tiller listens
      command: "kubectl patch deployment tiller-deploy --namespace=kube-system --type=json --patch='[{\"op\": \"add\", \"path\": \"/spec/template/spec/containers/0/command\", \"value\": [\"/tiller\", \"--listen=127.0.0.1:44134\"]}]'"
      args:
        creates: ./helm_secured
      become: yes

    - name: Apply patch to limit where tiller listens
      command: "kubectl patch deployment tiller-deploy --namespace=kube-system --type=json --patch='[{\"op\": \"replace\", \"path\": \"/spec/template/spec/containers/0/command\", \"value\": [\"/tiller\", \"--listen=127.0.0.1:44134\"]}]'"
      args:
        creates: ./helm_secured
      become: yes

    - name: Wait for tiller pod to come up
      shell: kubectl get pods --all-namespaces -o json
      register: kubectl_get_pods
      until: kubectl_get_pods.stdout|from_json|json_query('items[*].status.phase')|unique == ["Running"]
      delay: 30
      retries: 10
      become: yes

    - name: Mark that helm is secured
      file:
        path: ./helm_secured
        state: touch

- name: Set up Persistent Volume Claims on master node
  hosts: jupyterhub_head_node
  become_user: root
  become: yes
  vars_files: variables/vars.yml
  tasks:
    - name: Create kubernetes pods for all nfs volumes
      command: "helm install --name= --set nfs.server={{ item }} --set nfs.path={{ volume_mount_directory }}/{{ local_pv_directory }} --set storageClass.name=nfs-client-{{ item | replace('.','-') }} stable/nfs-client-provisioner"
      args:
        creates: "./{{ item }}-pv-provisioned"
      loop: "{{ groups['jupyterhub_nodes'] }}"

    - name: Mark pods made
      command: "touch ./{{ item }}-pv-provisioned"
      args:
        creates: "./{{ item }}-pv-provisioned"
      loop: "{{ groups['jupyterhub_nodes'] }}"

    # Note: I dropped --namespace=pgdatabase from this when I gave up on the
    # postgres db, you can re-add it if the chart is fixed at a later time.
    - name: Create kubernetes pods for all ephemeral nfs volumes for database
      command: "helm install --set nfs.server={{ item }} --set nfs.path={{ ephemeral_volume_share_directory }} --set storageClass.name=db-eph-nfs-client-{{ item | replace('.','-') }} stable/nfs-client-provisioner"
      args:
        creates: "./eph-db-{{ item }}-pv-provisioned"
      loop: "{{ groups['jupyterhub_nodes'] }}"
      when: use_ephemeral_database

    - name: Mark pods made
      command: "touch ./eph-db-{{ item }}-pv-provisioned"
      args:
        creates: "./eph-db-{{ item }}-pv-provisioned"
      loop: "{{ groups['jupyterhub_nodes'] }}"
      when: use_ephemeral_database

    - name: Create kubernetes pods for all ephemeral nfs volumes in default namespace
      command: "helm install --set nfs.server={{ item }} --set nfs.path={{ ephemeral_volume_share_directory }} --set storageClass.name=eph-nfs-client-{{ item | replace('.','-') }} stable/nfs-client-provisioner"
      args:
        creates: "./eph-{{ item }}-pv-provisioned"
      loop: "{{ groups['jupyterhub_nodes'] }}"
      when: ephemeral_persistent_storage

    - name: Mark pods made
      command: "touch ./eph-{{ item }}-pv-provisioned"
      args:
        creates: "./eph-{{ item }}-pv-provisioned"
      loop: "{{ groups['jupyterhub_nodes'] }}"
      when: ephemeral_persistent_storage


- name: Set up JupyterHub on the master node
  hosts: jupyterhub_head_node
  become_user: root
  vars_files: variables/vars.yml
  tasks:
    - name: Get random seed value
      command: openssl rand -hex 32
      args:
        creates: ./config.yml
      register: random_seed

    - name: Generate random key for hub secret
      command: openssl rand -hex 32
      args:
        creates: ./config.yml
      register: random_hub_secret

    - name: Add claim info for shared jupyterhub data
      template:
        dest: ./jupyter-shared-config.yml
        src: roles/files/jupyter-shared-config.yml
        mode: "777"
      become: yes

    - name: Create claim.
      command: kubectl apply -f ./jupyter-shared-config.yml
      args:
        creates: ./config.yml
      become: yes

    - name: Create config.yml for JupyterHub helm chart
      template:
        dest: ./config.yml
        src: roles/files/config.yml
        mode: "777"
      become: yes
      when: random_seed['changed']

    - name: Add JupyterHub to helm repository
      command: "helm repo add {{ jupyterhub_name }} https://jupyterhub.github.io/helm-chart/"
      become: yes

    - name: Update helm repository
      command: helm repo update
      become: yes

    - name: Install JupyterHub chart
      command: "helm upgrade --install {{ jupyterhub_name }} jupyterhub/jupyterhub --version={{ jupyterhub_version }} --values ./config.yml"
      become: yes

    - name: Get Metal Load Balancer chart
      get_url:
        dest: ./metallb.yaml
        mode: "777"
        url: https://raw.githubusercontent.com/google/metallb/v0.8.1/manifests/metallb.yaml

    - name: Install Metal Load Balancer chart
      command: "kubectl apply -f metallb.yaml"
      become: yes

    - name: Create Bare Metal Load Balancer changes
      lineinfile:
        path: ./metal-load-balancer.yml
        line: "{{ item }}"
        create: yes
        state: present
      loop:
        - "apiVersion: v1"
        - "kind: ConfigMap"
        - "metadata:"
        - "  namespace: metallb-system"
        - "  name: config"
        - "data:"
        - "  config: |"
        - "    address-pools:"
        - "    - name: default"
        - "      protocol: layer2"
        - "      addresses:"

    - name: Add all address ranges to Bare Metal Load Balancer
      lineinfile:
        path: ./metal-load-balancer.yml
        line: "      - {{ item }}-{{ item }}"
        state: present
      loop: "{{ groups['jupyterhub_nodes'] }}"

    - name: Apply Updated Load Balancer settings
      command: "kubectl apply -f ./metal-load-balancer.yml"

# TODO: Launch
